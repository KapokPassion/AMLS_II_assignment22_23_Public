{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a4a95c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 3\n",
    "patch_size = 84\n",
    "stride = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb1cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gc\n",
    "import os\n",
    "from utils import convert_rgb_to_ycbcr, convert_ycbcr_to_rgb, convert_rgb_to_y, calc_psnr, ssim\n",
    "\n",
    "def save_h5(imgs_dir, phase, scale, patch_size, stride, batch_size = 100, cache_size=256):\n",
    "    print('counting total patch number, please wait')\n",
    "    total_patch_number = 0\n",
    "    for img in sorted(os.listdir(imgs_dir)):\n",
    "        hr = Image.open(os.path.join(imgs_dir, img)).convert('RGB')\n",
    "        total_patch_number += ((hr.width // scale - patch_size) // stride + 1) * ((hr.height // scale - patch_size) // stride + 1)\n",
    "    print('total patch number : ' + str(total_patch_number))\n",
    "\n",
    "    h5f = h5py.File(r'./Datasets/{}_X{}.h5'.format(phase, scale), 'w')\n",
    "    try:\n",
    "        # train: 110550 for X4; 221487 for X3; 558822 for X2\n",
    "        # valid: 14175 for X4; 28161 for X3; 71115 for X2\n",
    "        # The first dimension of chunks should be an integral multiple of the batch_size during training\n",
    "        hlset = h5f.create_dataset('lh', (total_patch_number, 2, patch_size, patch_size), maxshape=(None, 2, patch_size, patch_size), dtype='f', chunks = (cache_size, 2, patch_size, patch_size))\n",
    "        # write to file by batch to avoid OOM\n",
    "        idx = 0 # starting index of a batch\n",
    "        patch_number = 0 # patch number in a batch\n",
    "        total_number = 0 # total number to let me know when to stop training\n",
    "        batch_size = batch_size # better be a factor of image number\n",
    "        image_idx_of_batch = 0 # i\n",
    "        batch = 0\n",
    "        patches = [] # (patch_idx dimension, lr/hr Y_channel dimension, height dimension, width dimension)\n",
    "        if len(os.listdir(imgs_dir)) % batch_size != 0:\n",
    "            print('warning: ' + str(len(os.listdir(imgs_dir)) % batch_size) + \\\n",
    "                  ' images will not be used, check batch_size. len(): ' + str(len(os.listdir(imgs_dir))))\n",
    "            return\n",
    "        # maybe it is confusing, but sorted only called once\n",
    "        print('batch ' + str(batch + 1) + ' processing')\n",
    "        for img in sorted(os.listdir(imgs_dir)):\n",
    "            hr = Image.open(os.path.join(imgs_dir, img)).convert('RGB')\n",
    "            hr_width = hr.width\n",
    "            hr_height = hr.height\n",
    "            lr = hr.resize((hr_width // scale, hr_height // scale), resample=Image.BICUBIC)\n",
    "            hr = np.array(hr).astype(np.float32)\n",
    "            lr = np.array(lr).astype(np.float32)\n",
    "            hr = convert_rgb_to_y(hr)\n",
    "            lr = convert_rgb_to_y(lr)\n",
    "            for x in range(0, lr.shape[0] - patch_size + 1, stride):\n",
    "                for y in range(0, lr.shape[1] - patch_size + 1, stride):\n",
    "                    # Add HR patch and LR patch to patches.\n",
    "                    # Continued lr and hr are much more efficient for IO than seperate dataset I used before\n",
    "                    patches.append([np.pad(lr[x // scale:x // scale + patch_size // scale, y // scale:y // scale + patch_size // scale],\n",
    "                                           ((0, patch_size * (scale - 1) // scale),\n",
    "                                           (0, patch_size * (scale - 1) // scale))),\n",
    "                                    hr[x:x + patch_size, y:y + patch_size]])\n",
    "\n",
    "            if image_idx_of_batch < batch_size - 1:\n",
    "                image_idx_of_batch += 1\n",
    "            # write to h5file by batch\n",
    "            else:\n",
    "                patch_number = len(patches)\n",
    "                patches = np.array(patches)\n",
    "                patches = patches / 255\n",
    "                print(patches.shape)\n",
    "                # shuffle all patches in the batch, thus batch_size should be high for a good shuffle\n",
    "                shuffle_ix = np.random.permutation(np.arange(patch_number))\n",
    "                patches = patches[shuffle_ix]\n",
    "                hlset[idx : idx + patch_number] = patches\n",
    "\n",
    "                del patches\n",
    "                gc.collect()\n",
    "\n",
    "                patches = []\n",
    "                idx += patch_number\n",
    "                total_number += patch_number\n",
    "                patch_number = 0\n",
    "                image_idx_of_batch = 0\n",
    "                batch += 1\n",
    "                print('batch:' + str(batch) + ' of ' + str(len(os.listdir(imgs_dir)) // batch_size))\n",
    "        print(total_number)\n",
    "    except BaseException as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14548e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_h5(imgs_dir=r'./Datasets/DIV2K_train_HR', phase='train', scale=scale, patch_size=patch_size, stride=stride, batch_size=100, cache_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9db807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# gpu acc\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from models import FSRCNN, OFSRCNN\n",
    "from datasets import TrainDataset, EvalDataset\n",
    "from utils import AverageMeter, calc_psnr, ssim\n",
    "\n",
    "def train(scale, patch_size, model_name, num_epochs, continue_epoch=0, batch_size=256):\n",
    "\n",
    "    train_file = r'./Datasets/train_X{}.h5'.format(scale)\n",
    "    eval_file = r'./Datasets/valid_X{}.h5'.format(scale)\n",
    "    outputs_dir = r'./outputs'\n",
    "    log_dir = r'./log'\n",
    "    lr_1 = 1e-3\n",
    "    lr_2 = 1e-3\n",
    "    lr_3 = 1e-3\n",
    "    lr_4 = 1e-3\n",
    "    lr_5 = 1e-4\n",
    "\n",
    "    batch_size = 256\n",
    "    num_workers = 0\n",
    "    num_epochs = num_epochs\n",
    "    seed = 1\n",
    "    model_name = model_name\n",
    "    continue_epoch = continue_epoch # will load this epoch weight file to continue\n",
    "\n",
    "    if not os.path.exists(outputs_dir):\n",
    "        os.makedirs(outputs_dir)\n",
    "\n",
    "    # benckmark mode to acc\n",
    "    cudnn.benchmark = True\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "    model = FSRCNN(scale=scale).to(device) if model_name == 'FSRCNN' else OFSRCNN(scale=scale).to(device)\n",
    "\n",
    "    # plt\n",
    "    lossLog = []\n",
    "    psnrLog = []\n",
    "    ssimLog = []\n",
    "\n",
    "    if continue_epoch != 0:\n",
    "        model.load_state_dict(torch.load(os.path.join(outputs_dir, '{}_X{}_epoch_{}.pth'.format(model_name, scale, continue_epoch))))\n",
    "        lossLog = np.loadtxt(os.path.join(log_dir, '{}_X{}_lossLog.txt'.format(model_name, scale)))\n",
    "        lossLog = lossLog.tolist()\n",
    "        psnrLog = np.loadtxt(os.path.join(log_dir, '{}_X{}_psnrLog.txt'.format(model_name, scale)))\n",
    "        psnrLog = psnrLog.tolist()\n",
    "        ssimLog = np.loadtxt(os.path.join(log_dir, '{}_X{}_ssimLog.txt'.format(model_name, scale)))\n",
    "        ssimLog = ssimLog.tolist()\n",
    "\n",
    "\n",
    "    # loss MSE\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # opt\n",
    "    optimizer = optim.Adam(\n",
    "        [{'params': model.features.parameters(), 'lr': lr_1},\n",
    "         {'params': model.shrinking.parameters(), 'lr': lr_2},\n",
    "         {'params': model.mapping.parameters(), 'lr': lr_3},\n",
    "         {'params': model.expanding.parameters(), 'lr': lr_4},\n",
    "         {'params': model.deconv.parameters(), 'lr': lr_5},\n",
    "        ]) if model_name == 'FSRCNN' else optim.Adam(\n",
    "        [{'params': model.features.parameters(), 'lr': lr_1},\n",
    "         {'params': model.shrinking.parameters(), 'lr': lr_2},\n",
    "         {'params': model.mapping.parameters(), 'lr': lr_3},\n",
    "         {'params': model.expanding.parameters(), 'lr': lr_4},\n",
    "         {'params': model.upsample.parameters(), 'lr': lr_5},\n",
    "        ])\n",
    "\n",
    "    train_dataset = TrainDataset(h5_file=train_file, patch_size=patch_size, scale=scale)\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        # dataset has already shuffled\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    #     pin_memory=True,\n",
    "        drop_last=True)\n",
    "\n",
    "    eval_dataset = EvalDataset(h5_file=eval_file, patch_size=patch_size, scale=scale)\n",
    "    eval_dataloader = DataLoader(\n",
    "        dataset=eval_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=True)\n",
    "\n",
    "    # weights copy\n",
    "    best_psnr = 0.0\n",
    "    best_epoch = 0\n",
    "    if continue_epoch != 0:\n",
    "        model.load_state_dict(torch.load(os.path.join(outputs_dir, '{}_X{}_best.pth'.format(model_name, scale))))\n",
    "    best_weights = copy.deepcopy(model.state_dict())\n",
    "    best_psnr = max(psnrLog)\n",
    "    best_epoch = psnrLog.index(best_psnr) + 1\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(continue_epoch + 1, continue_epoch + num_epochs + 1):\n",
    "        since = time.time()\n",
    "        print('epoch: '+ str(epoch) + ' at ' + time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        epoch_losses = AverageMeter()\n",
    "\n",
    "        process = 0\n",
    "        for data in train_dataloader:\n",
    "            process += 1\n",
    "            print('\\r', '***training process of epoch {} : {:.2f}%***'.format(epoch, process / len(train_dataset) * batch_size * 100), end='')\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(inputs)\n",
    "\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            epoch_losses.update(loss.item(), len(inputs))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        lossLog.append(np.array(epoch_losses.avg))\n",
    "        print('')\n",
    "        print('train loss: ' + str(epoch_losses.avg))\n",
    "        np.savetxt(os.path.join(log_dir, '{}_X{}_lossLog.txt'.format(model_name, scale)), lossLog)\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(outputs_dir, '{}_X{}_epoch_{}.pth'.format(model_name, scale, epoch)))\n",
    "\n",
    "        # PSNR SSIM\n",
    "        model.eval()\n",
    "        epoch_psnr = AverageMeter()\n",
    "        epoch_ssim = AverageMeter()\n",
    "\n",
    "        process = 0\n",
    "        for data in eval_dataloader:\n",
    "            process += 1\n",
    "            print('\\r', '***eval process of epoch {} : {:.2f}%***'.format(epoch, process / len(eval_dataset) * batch_size * 100), end='')\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds = model(inputs).clamp(0.0, 1.0)\n",
    "    #             preds = model(inputs)\n",
    "\n",
    "            epoch_psnr.update(calc_psnr(preds, labels), len(inputs))\n",
    "            epoch_ssim.update(ssim(preds, labels), len(inputs))\n",
    "\n",
    "        print('')\n",
    "        print('eval psnr: {:.2f}'.format(epoch_psnr.avg))\n",
    "        print('eval ssim: {:.2f}'.format(epoch_ssim.avg))\n",
    "\n",
    "        psnrLog.append(Tensor.cpu(epoch_psnr.avg))\n",
    "        ssimLog.append(Tensor.cpu(epoch_ssim.avg))\n",
    "        np.savetxt(os.path.join(log_dir, '{}_X{}_psnrLog.txt'.format(model_name, scale)), psnrLog)\n",
    "        np.savetxt(os.path.join(log_dir, '{}_X{}_ssimLog.txt'.format(model_name, scale)), ssimLog)\n",
    "\n",
    "        # update weight\n",
    "        if epoch_psnr.avg > best_psnr:\n",
    "            best_epoch = epoch\n",
    "            best_psnr = epoch_psnr.avg\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print('best epoch: {}, psnr: {:.2f}'.format(best_epoch, best_psnr))\n",
    "\n",
    "        torch.save(best_weights, os.path.join(outputs_dir, '{}_X{}_best.pth'.format(model_name, scale)))\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Epoch complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        print('')\n",
    "\n",
    "    print('best epoch: {}, psnr: {:.2f}'.format(best_epoch, best_psnr))\n",
    "\n",
    "    torch.save(best_weights, os.path.join(outputs_dir, '{}_X{}_best.pth'.format(model_name, scale)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6da7be08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 101 at 21:50:59\n",
      " ***training process of epoch 101 : 99.98%***\n",
      "train loss: 0.0015020041297878963\n",
      " ***eval process of epoch 101 : 100.00%***\n",
      "eval psnr: 27.45\n",
      "eval ssim: 0.87\n",
      "best epoch: 101, psnr: 27.45\n",
      "Epoch complete in 2m 33s\n",
      "\n",
      "epoch: 102 at 21:53:32\n",
      " ***training process of epoch 102 : 99.98%***\n",
      "train loss: 0.001493445773827009\n",
      " ***eval process of epoch 102 : 100.00%***\n",
      "eval psnr: 27.45\n",
      "eval ssim: 0.87\n",
      "best epoch: 101, psnr: 27.45\n",
      "Epoch complete in 2m 34s\n",
      "\n",
      "epoch: 103 at 21:56:06\n",
      " ***training process of epoch 103 : 99.98%***\n",
      "train loss: 0.0014938050972545423\n",
      " ***eval process of epoch 103 : 100.00%***\n",
      "eval psnr: 27.45\n",
      "eval ssim: 0.87\n",
      "best epoch: 101, psnr: 27.45\n",
      "Epoch complete in 2m 37s\n",
      "\n",
      "epoch: 104 at 21:58:43\n",
      " ***training process of epoch 104 : 99.98%***\n",
      "train loss: 0.0014945881373270528\n",
      " ***eval process of epoch 104 : 100.00%***\n",
      "eval psnr: 27.45\n",
      "eval ssim: 0.87\n",
      "best epoch: 101, psnr: 27.45\n",
      "Epoch complete in 2m 32s\n",
      "\n",
      "epoch: 105 at 22:01:15\n",
      " ***training process of epoch 105 : 99.98%***\n",
      "train loss: 0.0014938991509426407\n",
      " ***eval process of epoch 105 : 100.00%***\n",
      "eval psnr: 27.45\n",
      "eval ssim: 0.87\n",
      "best epoch: 101, psnr: 27.45\n",
      "Epoch complete in 2m 35s\n",
      "\n",
      "epoch: 106 at 22:03:50\n",
      " ***training process of epoch 106 : 99.98%***\n",
      "train loss: 0.001493473467147893\n",
      " ***eval process of epoch 106 : 100.00%***\n",
      "eval psnr: 27.45\n",
      "eval ssim: 0.87\n",
      "best epoch: 101, psnr: 27.45\n",
      "Epoch complete in 2m 30s\n",
      "\n",
      "epoch: 107 at 22:06:21\n",
      " ***training process of epoch 107 : 99.98%***\n",
      "train loss: 0.0014938028675983148\n",
      " ***eval process of epoch 107 : 100.00%***\n",
      "eval psnr: 27.45\n",
      "eval ssim: 0.87\n",
      "best epoch: 101, psnr: 27.45\n",
      "Epoch complete in 2m 28s\n",
      "\n",
      "epoch: 108 at 22:08:49\n",
      " ***training process of epoch 108 : 99.98%***\n",
      "train loss: 0.0014943268758411696\n",
      " ***eval process of epoch 108 : 100.00%***\n",
      "eval psnr: 27.45\n",
      "eval ssim: 0.87\n",
      "best epoch: 108, psnr: 27.45\n",
      "Epoch complete in 2m 35s\n",
      "\n",
      "epoch: 109 at 22:11:23\n",
      " ***training process of epoch 109 : 99.98%***\n",
      "train loss: 0.0014935711850250688\n",
      " ***eval process of epoch 109 : 100.00%***\n",
      "eval psnr: 27.45\n",
      "eval ssim: 0.87\n",
      "best epoch: 108, psnr: 27.45\n",
      "Epoch complete in 2m 44s\n",
      "\n",
      "epoch: 110 at 22:14:08\n",
      " ***training process of epoch 110 : 99.98%***\n",
      "train loss: 0.0014930433469872943\n",
      " ***eval process of epoch 110 : 100.00%***\n",
      "eval psnr: 27.45\n",
      "eval ssim: 0.87\n",
      "best epoch: 108, psnr: 27.45\n",
      "Epoch complete in 2m 48s\n",
      "\n",
      "epoch: 111 at 22:16:55\n",
      " ***training process of epoch 111 : 99.98%***\n",
      "train loss: 0.0014931696691184247\n",
      " ***eval process of epoch 111 : 100.00%***\n",
      "eval psnr: 27.46\n",
      "eval ssim: 0.87\n",
      "best epoch: 111, psnr: 27.46\n",
      "Epoch complete in 2m 36s\n",
      "\n",
      "epoch: 112 at 22:19:31\n",
      " ***training process of epoch 112 : 99.98%***\n",
      "train loss: 0.0014945006192489555\n",
      " ***eval process of epoch 112 : 100.00%***\n",
      "eval psnr: 27.46\n",
      "eval ssim: 0.87\n",
      "best epoch: 112, psnr: 27.46\n",
      "Epoch complete in 2m 28s\n",
      "\n",
      "epoch: 113 at 22:22:00\n",
      " ***training process of epoch 113 : 99.98%***\n",
      "train loss: 0.0014923621399726482\n",
      " ***eval process of epoch 113 : 100.00%***\n",
      "eval psnr: 27.46\n",
      "eval ssim: 0.87\n",
      "best epoch: 112, psnr: 27.46\n",
      "Epoch complete in 2m 28s\n",
      "\n",
      "epoch: 114 at 22:24:28\n",
      " ***training process of epoch 114 : 99.98%***\n",
      "train loss: 0.001492670606256522\n",
      " ***eval process of epoch 114 : 100.00%***\n",
      "eval psnr: 27.46\n",
      "eval ssim: 0.87\n",
      "best epoch: 112, psnr: 27.46\n",
      "Epoch complete in 2m 28s\n",
      "\n",
      "epoch: 115 at 22:26:55\n",
      " ***training process of epoch 115 : 99.98%***\n",
      "train loss: 0.0014959932218111965\n",
      " ***eval process of epoch 115 : 100.00%***\n",
      "eval psnr: 27.46\n",
      "eval ssim: 0.87\n",
      "best epoch: 115, psnr: 27.46\n",
      "Epoch complete in 2m 32s\n",
      "\n",
      "epoch: 116 at 22:29:27\n",
      " ***training process of epoch 116 : 99.98%***\n",
      "train loss: 0.0014913205354813636\n",
      " ***eval process of epoch 116 : 100.00%***\n",
      "eval psnr: 27.46\n",
      "eval ssim: 0.87\n",
      "best epoch: 115, psnr: 27.46\n",
      "Epoch complete in 2m 29s\n",
      "\n",
      "epoch: 117 at 22:31:56\n",
      " ***training process of epoch 117 : 99.98%***\n",
      "train loss: 0.0014927272388114018\n",
      " ***eval process of epoch 117 : 100.00%***\n",
      "eval psnr: 27.46\n",
      "eval ssim: 0.87\n",
      "best epoch: 115, psnr: 27.46\n",
      "Epoch complete in 2m 31s\n",
      "\n",
      "epoch: 118 at 22:34:28\n",
      " ***training process of epoch 118 : 99.98%***\n",
      "train loss: 0.001492507728368496\n",
      " ***eval process of epoch 118 : 100.00%***\n",
      "eval psnr: 27.46\n",
      "eval ssim: 0.87\n",
      "best epoch: 115, psnr: 27.46\n",
      "Epoch complete in 2m 31s\n",
      "\n",
      "epoch: 119 at 22:36:59\n",
      " ***training process of epoch 119 : 99.98%***\n",
      "train loss: 0.0014927932837805738\n",
      " ***eval process of epoch 119 : 100.00%***\n",
      "eval psnr: 27.46\n",
      "eval ssim: 0.87\n",
      "best epoch: 115, psnr: 27.46\n",
      "Epoch complete in 2m 31s\n",
      "\n",
      "epoch: 120 at 22:39:30\n",
      " ***training process of epoch 120 : 99.98%***\n",
      "train loss: 0.0014925656229865904\n",
      " ***eval process of epoch 120 : 100.00%***\n",
      "eval psnr: 27.46\n",
      "eval ssim: 0.87\n",
      "best epoch: 115, psnr: 27.46\n",
      "Epoch complete in 2m 33s\n",
      "\n",
      "best epoch: 115, psnr: 27.46\n"
     ]
    }
   ],
   "source": [
    "train(scale=scale, patch_size=patch_size, model_name='FSRCNN', num_epochs=20, continue_epoch=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36785df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from models import OFSRCNN, FSRCNN\n",
    "from utils import convert_rgb_to_ycbcr, convert_ycbcr_to_rgb, calc_psnr, ssim\n",
    "\n",
    "scale = 3\n",
    "model_name = 'FSRCNN'\n",
    "dataset_name = 'Set5'\n",
    "\n",
    "def test(scale, model_name, dataset_name):\n",
    "\n",
    "    weight_dir = os.path.join('./outputs/', '{}_X{}_best.pth'.format(model_name, scale))\n",
    "    hr_dir = './Datasets/' + dataset_name + '/HR'\n",
    "    out_dir = './Datasets/' + dataset_name + '/' + model_name\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    model = FSRCNN(scale=scale).to(device) if model_name == 'FSRCNN' else OFSRCNN(scale=scale).to(device)\n",
    "    model.load_state_dict(torch.load(weight_dir))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if not os.path.exists(os.path.join(out_dir, 'X{}'.format(scale))):\n",
    "        os.makedirs(os.path.join(out_dir, 'X{}'.format(scale)))\n",
    "\n",
    "    for img in sorted(os.listdir(hr_dir)):\n",
    "        image = Image.open(os.path.join(hr_dir, img)).convert('RGB')\n",
    "        image_lr = image.resize((image.width // scale, image.height // scale), resample=Image.BICUBIC)\n",
    "\n",
    "        # image to ycbcr arr\n",
    "        image_arr = np.array(image).astype(np.float32)\n",
    "        ycbcr = convert_rgb_to_ycbcr(image_arr)\n",
    "        # y of ycbcr\n",
    "        y = ycbcr[..., 0]\n",
    "        y /= 255.\n",
    "        y = torch.from_numpy(y).to(device)\n",
    "        y = y.unsqueeze(0).unsqueeze(0) # dim expand\n",
    "\n",
    "        # lr image to ycbcr arr\n",
    "        image_lr = np.array(image_lr).astype(np.float32)\n",
    "        ycbcr_lr = convert_rgb_to_ycbcr(image_lr)\n",
    "        # y of lr ycbcr\n",
    "        y_lr = ycbcr_lr[..., 0]\n",
    "        y_lr /= 255.\n",
    "        y_lr = torch.from_numpy(y_lr).to(device)\n",
    "        y_lr = y_lr.unsqueeze(0).unsqueeze(0) # dim expand\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(y_lr).clamp(0.0, 1.0)\n",
    "\n",
    "        # resize preds if necessary\n",
    "        if preds.size()[2] != image.height or preds.size()[3] != image.width:\n",
    "            temp_image = Image.fromarray(preds.numpy().squeeze(0).squeeze(0))\n",
    "            temp_image = temp_image.resize((image.width, image.height), resample=Image.BICUBIC)\n",
    "            temp_image_arr = np.array(temp_image).astype(np.float32)\n",
    "            preds = torch.from_numpy(temp_image_arr).to(device).unsqueeze(0).unsqueeze(0)\n",
    "        print(img)\n",
    "        print('PSNR on y: {:.4f}'.format(calc_psnr(y, preds)))\n",
    "        print('SSIM on y: {:.4f}'.format(ssim(y, preds)))\n",
    "\n",
    "        preds = preds.mul(255.0).numpy().squeeze(0).squeeze(0)\n",
    "\n",
    "        # (channels,imagesize,imagesize) to (imagesize,imagesize,channels)\n",
    "        output = np.array([preds, ycbcr[..., 1], ycbcr[..., 2]], dtype=object).transpose([1, 2, 0])\n",
    "\n",
    "        output = np.clip(convert_ycbcr_to_rgb(output), 0.0, 255.0).astype(np.uint8)\n",
    "        output = Image.fromarray(output)\n",
    "\n",
    "        output.save(os.path.join(out_dir, 'X{}'.format(scale), img))\n",
    "    print('test done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a73556",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(scale=scale, model_name='OFSRCNN', dataset_name='Set5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
